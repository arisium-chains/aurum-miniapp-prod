version: '3.8'

services:
  # Main Next.js application
  app:
    build: .
    ports:
      - '3000:3000'
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - FACE_DETECTION_SERVICE=http://face-detection-service:8001
      - FACE_EMBEDDING_SERVICE=http://face-embedding-service:8002
    depends_on:
      - redis
      - ml-api
      - face-detection-service
      - face-embedding-service
    volumes:
      - ./public/models:/app/public/models
    restart: unless-stopped

  # Redis for job queue and caching (shared between services)
  redis:
    image: redis:7-alpine
    ports:
      - '6381:6379'
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant
    ports:
      - '6334:6333'
      - '6335:6334'
    volumes:
      - qdrant_storage:/qdrant/storage
    restart: unless-stopped

  # ML Face Scoring API Service (using standalone ml-api)
  ml-api:
    build:
      context: ../ml-api
      dockerfile: Dockerfile
    ports:
      - '3001:3000'
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - PORT=3000
      - FACE_DETECTION_SERVICE=http://face-detection-service:8001
      - FACE_EMBEDDING_SERVICE=http://face-embedding-service:8002
    depends_on:
      - redis
      - face-detection-service
      - face-embedding-service
    volumes:
      - ../ml-api/temp:/app/temp
      - ../ml-api/models:/app/models
    restart: unless-stopped

  # ML Worker for processing face scoring jobs (using standalone ml-api)
  ml-worker:
    build:
      context: ../ml-api
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - FACE_DETECTION_SERVICE=http://face-detection-service:8001
      - FACE_EMBEDDING_SERVICE=http://face-embedding-service:8002
    depends_on:
      - redis
      - face-detection-service
      - face-embedding-service
    volumes:
      - ../ml-api/temp:/app/temp
      - ../ml-api/models:/app/models
    command: node dist/worker.js
    restart: unless-stopped

  # Rust ML Services
  face-detection-service:
    build:
      context: ../../aurum-ml-services
      dockerfile: face-detection/Dockerfile
    ports:
      - '8001:8001'
    environment:
      - PORT=8001
      - MODEL_PATH=/app/models/face_detection_model.onnx
      - LOG_LEVEL=info
    volumes:
      - ./public/models:/app/models
    restart: unless-stopped
    depends_on:
      - redis

  face-embedding-service:
    build:
      context: ../../aurum-ml-services
      dockerfile: face-embedding/Dockerfile
    ports:
      - '8002:8002'
    environment:
      - PORT=8002
      - MODEL_PATH=/app/models/face_embedding_model.onnx
      - LOG_LEVEL=info
    volumes:
      - ./public/models:/app/models
    restart: unless-stopped
    depends_on:
      - redis

  # Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    ports:
      - '80:80'
      - '443:443'
    volumes:
      - ../nginx/conf:/etc/nginx/conf.d
      - ../nginx/certs:/etc/nginx/certs
    depends_on:
      - app
    restart: unless-stopped

volumes:
  redis_data:
  qdrant_storage:
