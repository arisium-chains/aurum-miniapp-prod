services:
  # Main Next.js application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - FACE_DETECTION_SERVICE=http://face-detection-service:8001
      - FACE_EMBEDDING_SERVICE=http://face-embedding-service:8002
      - PORT=3000
    depends_on:
      redis:
        condition: service_healthy
      ml-api:
        condition: service_healthy
      face-detection-service:
        condition: service_healthy
      face-embedding-service:
        condition: service_healthy
    volumes:
      - ./public/models:/app/public/models:ro
      - app_logs:/app/data/logs
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:3000/api/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Node.js Scoring Service
  scoring-service:
    build:
      context: .
      dockerfile: Dockerfile.scoring-service
    ports:
      - "3002:3000"
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - PORT=3000
      - HOSTNAME=0.0.0.0
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - scoring_logs:/app/data/logs
      - scoring_models:/app/data/models
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:3000/api/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Redis for job queue and caching (shared between services)
  redis:
    image: redis:7-alpine
    ports:
      - "6381:6379"
    volumes:
      - redis_data:/data
      - redis_config:/etc/redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:v1.7.0
    ports:
      - "6334:6333"
      - "6335:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
      - qdrant_config:/qdrant/config
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY:-}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ML Face Scoring API Service
  ml-api:
    build:
      context: ./ml-face-score-api
      dockerfile: Dockerfile
    ports:
      - "3001:3000"
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - PORT=3000
      - FACE_DETECTION_SERVICE=http://face-detection-service:8001
      - FACE_EMBEDDING_SERVICE=http://face-embedding-service:8002
    depends_on:
      redis:
        condition: service_healthy
      face-detection-service:
        condition: service_healthy
      face-embedding-service:
        condition: service_healthy
    volumes:
      - ./ml-face-score-api/temp:/app/temp
      - ./ml-face-score-api/models:/app/models:ro
      - ml_api_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:3000/api/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ML Worker for processing face scoring jobs
  ml-worker:
    build:
      context: ./ml-face-score-api
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - FACE_DETECTION_SERVICE=http://face-detection-service:8001
      - FACE_EMBEDDING_SERVICE=http://face-embedding-service:8002
    depends_on:
      redis:
        condition: service_healthy
      face-detection-service:
        condition: service_healthy
      face-embedding-service:
        condition: service_healthy
    volumes:
      - ./ml-face-score-api/temp:/app/temp
      - ./ml-face-score-api/models:/app/models:ro
      - ml_worker_logs:/app/logs
    command: node dist/worker.js
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "-f", "worker.js"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Rust ML Services
  face-detection-service:
    build:
      context: /Users/poomcryptoman/Arisium/aurum-circle-new/aurum-ml-services
      dockerfile: face-detection/Dockerfile
    ports:
      - "8001:8001"
    environment:
      - PORT=8001
      - MODEL_PATH=/app/models/face_detection_model.onnx
      - LOG_LEVEL=info
      - RUST_LOG=info
    volumes:
      - ./public/models:/app/models:ro
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  face-embedding-service:
    build:
      context: /Users/poomcryptoman/Arisium/aurum-circle-new/aurum-ml-services
      dockerfile: face-embedding/Dockerfile
    ports:
      - "8002:8002"
    environment:
      - PORT=8002
      - MODEL_PATH=/app/models/face_embedding_model.onnx
      - LOG_LEVEL=info
      - RUST_LOG=info
    volumes:
      - ./public/models:/app/models:ro
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/conf:/etc/nginx/conf.d:ro
      - ./nginx/certs:/etc/nginx/certs:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      app:
        condition: service_healthy
      scoring-service:
        condition: service_healthy
      ml-api:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/nginx-health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  redis_data:
  redis_config:
  qdrant_storage:
  qdrant_config:
  app_logs:
  scoring_logs:
  scoring_models:
  ml_api_logs:
  ml_worker_logs:
  nginx_logs:
